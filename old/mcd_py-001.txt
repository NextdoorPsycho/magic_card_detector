# File: lib/core/recognition.py
import numpy as np
import imagehash
from PIL import Image as PILImage
import cv2
from scipy.ndimage import rotate
def phash_diff(reference_images, phash_im):
    
    diff = np.zeros(len(reference_images))
    for i, ref_im in enumerate(reference_images):
        diff[i] = phash_im - ref_im.phash
    return diff
def phash_compare(im_seg, reference_images, hash_separation_thr=4.0, verbose=False):
    
    card_name = 
    is_recognized = False
    recognition_score = 0.
    rotations = np.array([0., 90., 180., 270.])
    d_0_dist = np.zeros(len(rotations))
    d_0 = np.zeros((len(reference_images), len(rotations)))
    for j, rot in enumerate(rotations):
        if not -1.e-5 < rot < 1.e-5:
            phash_im = imagehash.phash(
                PILImage.fromarray(np.uint8(255 * cv2.cvtColor(
                    rotate(im_seg, rot), cv2.COLOR_BGR2RGB))),
                hash_size=32)
        else:
            phash_im = imagehash.phash(
                PILImage.fromarray(np.uint8(255 * cv2.cvtColor(
                    im_seg, cv2.COLOR_BGR2RGB))),
                hash_size=32)
        d_0[:, j] = phash_diff(reference_images, phash_im)
        d_0_ = d_0[d_0[:, j] > np.amin(d_0[:, j]), j]
        d_0_ave = np.average(d_0_)
        d_0_std = np.std(d_0_)
        d_0_dist[j] = (d_0_ave - np.amin(d_0[:, j])) / d_0_std
        if verbose:
            print( + str(d_0_dist[j]))
        if (d_0_dist[j] > hash_separation_thr and
                np.argmax(d_0_dist) == j):
            card_name = reference_images[np.argmin(d_0[:, j])]\
                .name.split()[0]
            is_recognized = True
            recognition_score = d_0_dist[j] / hash_separation_thr
            break
    return (is_recognized, recognition_score, card_name)# File: lib/core/detector.py
import glob
import os
import pickle
import cv2
from lib.models import ReferenceImage, TestImage
from lib.image import segment_image
from lib.core.recognition import phash_compare
class MagicCardDetector:
    
    def __init__(self, output_path=None):
        
        self.output_path = output_path
        self.reference_images = []
        self.test_images = []
        self.verbose = False
        self.visual = False
        self.hash_separation_thr = 4.0
        self.thr_lvl = 70
        self.clahe = cv2.createCLAHE(clipLimit=2.0,
                                    tileGridSize=(8, 8))
    def export_reference_data(self, path):
        
        hlist = []
        for image in self.reference_images:
            hlist.append(ReferenceImage(image.name,
                                       None,
                                       None,
                                       image.phash))
        with open(path, ) as fhandle:
            pickle.dump(hlist, fhandle)
    def read_prehashed_reference_data(self, path):
        
        print( + str(path))
        print(, end=)
        with open(path, ) as filename:
            hashed_list = pickle.load(filename)
        for ref_im in hashed_list:
            self.reference_images.append(
                ReferenceImage(ref_im.name, None, self.clahe, ref_im.phash))
        print()
    def read_and_adjust_reference_images(self, path):
        
        print( + str(path))
        print(, end=)
        filenames = glob.glob(path + )
        for filename in filenames:
            img = cv2.imread(filename)
            img_name = filename.split(path)[1]
            self.reference_images.append(
                ReferenceImage(img_name, img, self.clahe))
        print()
    def read_and_adjust_test_images(self, path):
        
        maxsize = 1000
        print( + str(path))
        print(, end=)
        filenames = glob.glob(path.rstrip() + )
        for filename in filenames:
            img = cv2.imread(filename)
            if min(img.shape[0], img.shape[1]) > maxsize:
                scalef = maxsize / min(img.shape[0], img.shape[1])
                img = cv2.resize(img,
                                (int(img.shape[1] * scalef),
                                 int(img.shape[0] * scalef)),
                                interpolation=cv2.INTER_AREA)
            img_name = os.path.basename(filename)
            self.test_images.append(
                TestImage(img_name, img, self.clahe))
        print()
    def recognize_segment(self, image_segment):
        
        return phash_compare(image_segment, self.reference_images, 
                           self.hash_separation_thr, self.verbose)
    def recognize_cards_in_image(self, test_image, contouring_mode):
        
        print()
        print( + str(contouring_mode) + )
        test_image.candidate_list.clear()
        segment_image(test_image, contouring_mode=contouring_mode, 
                    visual=self.visual, verbose=self.verbose)
        print( +
              str(len(test_image.candidate_list)) + )
        print()
        for i_cand, candidate in enumerate(test_image.candidate_list):
            im_seg = candidate.image
            if self.verbose:
                print(str(i_cand + 1) + " / " +
                      str(len(test_image.candidate_list)))
            # Easy fragment / duplicate detection
            for other_candidate in test_image.candidate_list:
                if (other_candidate.is_recognized and
                        not other_candidate.is_fragment):
                    if other_candidate.contains(candidate):
                        candidate.is_fragment = True
            if not candidate.is_fragment:
                (candidate.is_recognized,
                 candidate.recognition_score,
                 candidate.name) = self.recognize_segment(im_seg)
        print( +
              str(len(test_image.return_recognized())) +
              )
        if self.verbose:
            for card in test_image.return_recognized():
                print(card.name +  + str(card.recognition_score))
        print()
        # Final fragment detection
        test_image.mark_fragments()
        print()
    def run_recognition(self, image_index=None):
        
        if image_index is None:
            image_index = range(len(self.test_images))
        elif not isinstance(image_index, list):
            image_index = [image_index]
        for i in image_index:
            test_image = self.test_images[i]
            print( + test_image.name)
            if self.visual:
                import matplotlib.pyplot as plt
                print()
                plt.imshow(cv2.cvtColor(test_image.original,
                                       cv2.COLOR_BGR2RGB))
                plt.show()
            alg_list = [, ]
            for alg in alg_list:
                self.recognize_cards_in_image(test_image, alg)
                test_image.discard_unrecognized_candidates()
                if (not test_image.may_contain_more_cards() or
                        len(test_image.return_recognized()) > 5):
                    break
            print()
            test_image.plot_image_with_recognized(self.output_path, self.visual)
            print()
            test_image.print_recognized()
        print()# File: lib/core/__init__.py
from lib.core.detector import MagicCardDetector
from lib.core.recognition import phash_compare, phash_diff# File: lib/bin/__init__.py
# File: lib/bin/detect_cards.py
import os
import argparse
import io
import cProfile
import pstats
from lib import MagicCardDetector
def main():
    
    global profiler
    parser = argparse.ArgumentParser(
        description= +
                    )
    parser.add_argument(,
                       help=)
    parser.add_argument(,
                       help=)
    parser.add_argument(, , default=,
                       help=)
    parser.add_argument(, , action=,
                       help=)
    parser.add_argument(, , action=,
                       help=)
    parser.add_argument(, action=,
                       help=)
    parser.add_argument(, , nargs=,
                       help=)
    parser.add_argument(, , type=float, default=4.0,
                       help=)
    args = parser.parse_args()
    # Create the output path
    output_path = args.output_path.rstrip()
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    # Check if input path exists
    if not os.path.isdir(args.input_path):
        print(f"Error: Input path  does not exist or is not a directory.")
        return 1
    # Check if hash file exists
    if not os.path.isfile(args.phash):
        print(f"Error: Hash file  does not exist.")
        return 1
    # Instantiate the detector
    card_detector = MagicCardDetector(output_path)
    card_detector.visual = args.visual
    card_detector.verbose = args.verbose
    card_detector.hash_separation_thr = args.threshold
    # Read the reference data
    try:
        card_detector.read_prehashed_reference_data(args.phash)
    except Exception as e:
        print(f"Error loading hash data: {e}")
        return 1
    # Read test images
    try:
        card_detector.read_and_adjust_test_images(args.input_path)
        if not card_detector.test_images:
            print(f"No images found in {args.input_path}")
            return 1
    except Exception as e:
        print(f"Error loading test images: {e}")
        return 1
    # If specific images are requested, find their indices
    image_indices = None
    if args.images:
        image_indices = []
        for image_name in args.images:
            found = False
            for i, test_image in enumerate(card_detector.test_images):
                if test_image.name == image_name or image_name in test_image.name:
                    image_indices.append(i)
                    found = True
            if not found:
                print(f"Warning: Image  not found.")
    # Start profiling if requested
    if args.profile:
        profiler = cProfile.Profile()
        profiler.enable()
    # Run the card detection and recognition
    try:
        card_detector.run_recognition(image_indices)
    except Exception as e:
        print(f"Error during card recognition: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1
    # Stop profiling and print results if requested
    if args.profile:
        profiler.disable()
        profiler.dump_stats()
        profiler_stream = io.StringIO()
        sortby = pstats.SortKey.CUMULATIVE
        profiler_stats = pstats.Stats(
            profiler, stream=profiler_stream).sort_stats(sortby)
        profiler_stats.print_stats(20)
        print(profiler_stream.getvalue())
    return 0
if __name__ == "__main__":
    exit(main())# File: lib/bin/generate_hashes.py
import argparse
import os
from lib import MagicCardDetector
def main():
    
    parser = argparse.ArgumentParser(
        description= +
                   )
    parser.add_argument(, , required=True,
                      help=)
    parser.add_argument(, , required=True,
                      help=)
    parser.add_argument(, , action=,
                      help=)
    args = parser.parse_args()
    # Validate the set path
    set_path = args.set_path
    if not os.path.isdir(set_path):
        print(f"Error: Set path  is not a valid directory.")
        return 1
    
    # Add trailing slash if needed
    if not set_path.endswith():
        set_path += 
    # Create the output directory if it doesn't exist
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    print(f"Generating hashes for card images in {set_path}")
    print(f"Output will be saved to {args.output}")
    # Create detector and generate hashes
    detector = MagicCardDetector()
    detector.verbose = args.verbose
    
    # Load reference images and generate hashes
    try:
        detector.read_and_adjust_reference_images(set_path)
        
        if not detector.reference_images:
            print(f"No .jpg images found in {set_path}")
            return 1
            
        print(f"Found {len(detector.reference_images)} images")
        
        # Export the hash data
        detector.export_reference_data(args.output)
        print(f"Hash data successfully saved to {args.output}")
        
        return 0
    except Exception as e:
        print(f"Error generating hashes: {e}")
        return 1
if __name__ == "__main__":
    main()# File: lib/__init__.py
from lib.core.detector import MagicCardDetector
__version__ = "1.0.0"# File: lib/utils/config.py
DEFAULT_HASH_SEPARATION_THRESHOLD = 4.0
DEFAULT_THRESHOLD_LEVEL = 70
DEFAULT_MAX_IMAGE_SIZE = 1000# File: lib/utils/__init__.py
from lib.utils.config import (
    DEFAULT_HASH_SEPARATION_THRESHOLD,
    DEFAULT_THRESHOLD_LEVEL,
    DEFAULT_MAX_IMAGE_SIZE,
)# File: lib/models/__init__.py
from lib.models.card import CardCandidate
from lib.models.image import ReferenceImage, TestImage# File: lib/models/card.py
import numpy as np
from dataclasses import dataclass
from shapely.geometry.polygon import Polygon
@dataclass
class CardCandidate:
    
    image: np.ndarray
    bounding_quad: Polygon
    image_area_fraction: float
    is_recognized: bool = False
    recognition_score: float = 0.
    is_fragment: bool = False
    name: str = 
    def contains(self, other):
        
        return bool(other.bounding_quad.within(self.bounding_quad) and
                    other.name == self.name)# File: lib/models/image.py
import io
from copy import deepcopy
from itertools import product
import cv2
import imagehash
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image as PILImage
from shapely.geometry.polygon import Polygon
class ReferenceImage:
    
    def __init__(self, name, original_image, clahe, phash=None):
        self.name = name
        self.original = original_image
        self.clahe = clahe
        self.adjusted = None
        self.phash = phash
        if self.original is not None:
            self.histogram_adjust()
            self.calculate_phash()
    def calculate_phash(self):
        
        self.phash = imagehash.phash(
            PILImage.fromarray(np.uint8(255 * cv2.cvtColor(
                self.adjusted, cv2.COLOR_BGR2RGB))),
            hash_size=32)
    def histogram_adjust(self):
        
        lab = cv2.cvtColor(self.original, cv2.COLOR_BGR2LAB)
        lightness, redness, yellowness = cv2.split(lab)
        corrected_lightness = self.clahe.apply(lightness)
        limg = cv2.merge((corrected_lightness, redness, yellowness))
        self.adjusted = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
class TestImage:
    
    def __init__(self, name, original_image, clahe):
        self.name = name
        self.original = original_image
        self.clahe = clahe
        self.adjusted = None
        self.phash = None
        self.visual = False
        self.histogram_adjust()
        # self.calculate_phash()
        self.candidate_list = []
    def histogram_adjust(self):
        
        lab = cv2.cvtColor(self.original, cv2.COLOR_BGR2LAB)
        lightness, redness, yellowness = cv2.split(lab)
        corrected_lightness = self.clahe.apply(lightness)
        limg = cv2.merge((corrected_lightness, redness, yellowness))
        self.adjusted = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
    def mark_fragments(self):
        
        for (candidate, other_candidate) in product(self.candidate_list,
                                                   repeat=2):
            if candidate.is_fragment or other_candidate.is_fragment:
                continue
            if ((candidate.is_recognized or other_candidate.is_recognized) and
                    candidate is not other_candidate):
                i_area = candidate.bounding_quad.intersection(
                    other_candidate.bounding_quad).area
                min_area = min(candidate.bounding_quad.area,
                               other_candidate.bounding_quad.area)
                if i_area > 0.5 * min_area:
                    if (candidate.is_recognized and
                            other_candidate.is_recognized):
                        if (candidate.recognition_score <
                                other_candidate.recognition_score):
                            candidate.is_fragment = True
                        else:
                            other_candidate.is_fragment = True
                    else:
                        if candidate.is_recognized:
                            other_candidate.is_fragment = True
                        else:
                            candidate.is_fragment = True
    def plot_image_with_recognized(self, output_path=None, visual=False):
        
        # Plotting
        plt.figure()
        plt.imshow(cv2.cvtColor(self.original, cv2.COLOR_BGR2RGB))
        plt.axis()
        for candidate in self.candidate_list:
            if not candidate.is_fragment:
                full_image = self.adjusted
                bquad_corners = np.empty((4, 2))
                bquad_corners[:, 0] = np.asarray(
                    candidate.bounding_quad.exterior.coords)[:-1, 0]
                bquad_corners[:, 1] = np.asarray(
                    candidate.bounding_quad.exterior.coords)[:-1, 1]
                plt.plot(np.append(bquad_corners[:, 0],
                                   bquad_corners[0, 0]),
                         np.append(bquad_corners[:, 1],
                                   bquad_corners[0, 1]), )
                bounding_poly = Polygon([[x, y] for (x, y) in
                                         zip(bquad_corners[:, 0],
                                             bquad_corners[:, 1])])
                fntsze = int(6 * bounding_poly.length / full_image.shape[1])
                bbox_color =  if candidate.is_recognized else 
                plt.text(np.average(bquad_corners[:, 0]),
                         np.average(bquad_corners[:, 1]),
                         candidate.name.capitalize(),
                         horizontalalignment=,
                         fontsize=fntsze,
                         bbox=dict(facecolor=bbox_color,
                                   alpha=0.7))
        if output_path is not None:
            plt.savefig(output_path +  +
                        str(self.name.split()[0]) +
                        , dpi=600)
        if visual:
            plt.show()
        # Save figure to a bytes buffer
        buf = io.BytesIO()
        # Save as PNG for transparency support if needed, or JPG for size
        plt.savefig(buf, format=, bbox_inches=, pad_inches=0)
        plt.close() # Close the plot figure to free memory
        buf.seek(0)
        img_bytes = buf.getvalue()
        buf.close()
        return img_bytes
    def print_recognized(self):
        
        recognized_list = self.return_recognized()
        print( +
              str(len(recognized_list)) +
              )
        for card in recognized_list:
            print(card.name +
                   +
                  str(card.recognition_score))
    def return_recognized(self):
        
        recognized_list = []
        for candidate in self.candidate_list:
            if candidate.is_recognized and not candidate.is_fragment:
                recognized_list.append(candidate)
        return recognized_list
    def discard_unrecognized_candidates(self):
        
        recognized_list = deepcopy(self.return_recognized())
        self.candidate_list.clear()
        self.candidate_list = recognized_list
    def may_contain_more_cards(self):
        
        recognized_list = self.return_recognized()
        if not recognized_list:
            return True
        tot_area = 0.
        min_area = 1.
        for card in recognized_list:
            tot_area += card.image_area_fraction
            if card.image_area_fraction < min_area:
                min_area = card.image_area_fraction
        return bool(tot_area + 1.5 * min_area < 1.)# File: lib/image/__init__.py
from lib.image.processing import (
    contour_image_gray,
    contour_image_rgb,
    contour_image,
    segment_image,
)# File: lib/image/processing.py
import numpy as np
import cv2
from shapely.affinity import scale
from lib.geometry import four_point_transform, characterize_card_contour
from lib.models import CardCandidate
def contour_image_gray(full_image, thresholding=, visual=False, verbose=False):
    
    gray = cv2.cvtColor(full_image, cv2.COLOR_BGR2GRAY)
    if thresholding == :
        fltr_size = 1 + 2 * (min(full_image.shape[0],
                               full_image.shape[1]) // 20)
        thresh = cv2.adaptiveThreshold(gray,
                                       255,
                                       cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY,
                                       fltr_size,
                                       10)
    else:
        _, thresh = cv2.threshold(gray,
                                  70,
                                  255,
                                  cv2.THRESH_BINARY)
    if visual and verbose:
        import matplotlib.pyplot as plt
        plt.imshow(thresh)
        plt.show()
    contours, _ = cv2.findContours(
        np.uint8(thresh), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    return contours
def contour_image_rgb(full_image, clahe, visual=False, verbose=False):
    
    blue, green, red = cv2.split(full_image)
    blue = clahe.apply(blue)
    green = clahe.apply(green)
    red = clahe.apply(red)
    _, thr_b = cv2.threshold(blue, 110, 255, cv2.THRESH_BINARY)
    _, thr_g = cv2.threshold(green, 110, 255, cv2.THRESH_BINARY)
    _, thr_r = cv2.threshold(red, 110, 255, cv2.THRESH_BINARY)
    contours_b, _ = cv2.findContours(
        np.uint8(thr_b), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    contours_g, _ = cv2.findContours(
        np.uint8(thr_g), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    contours_r, _ = cv2.findContours(
        np.uint8(thr_r), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    contours = contours_b + contours_g + contours_r
    if visual and verbose:
        import matplotlib.pyplot as plt
        plt.imshow(thr_r)
        plt.show()
        plt.imshow(thr_g)
        plt.show()
        plt.imshow(thr_b)
        plt.show()
    return contours
def contour_image(full_image, mode=, clahe=None, visual=False, verbose=False):
    
    if mode == :
        contours = contour_image_gray(full_image,
                                    thresholding=,
                                    visual=visual,
                                    verbose=verbose)
    elif mode == :
        contours = contour_image_gray(full_image,
                                    thresholding=,
                                    visual=visual,
                                    verbose=verbose)
    elif mode == :
        if clahe is None:
            raise ValueError("CLAHE object is required for RGB mode")
        contours = contour_image_rgb(full_image, 
                                   clahe,
                                   visual=visual,
                                   verbose=verbose)
    elif mode == :
        contours = contour_image_gray(full_image,
                                    thresholding=,
                                    visual=visual,
                                    verbose=verbose)
        contours += contour_image_gray(full_image,
                                     thresholding=,
                                     visual=visual,
                                     verbose=verbose)
        if clahe is None:
            raise ValueError("CLAHE object is required for  mode")
        contours += contour_image_rgb(full_image, 
                                    clahe,
                                    visual=visual,
                                    verbose=verbose)
    else:
        raise ValueError()
    contours_sorted = sorted(contours, key=cv2.contourArea, reverse=True)
    return contours_sorted
def segment_image(test_image, contouring_mode=, visual=False, verbose=False):
    
    full_image = test_image.adjusted.copy()
    image_area = full_image.shape[0] * full_image.shape[1]
    max_segment_area = 0.01  # largest card area
    contours = contour_image(full_image, 
                           mode=contouring_mode, 
                           clahe=test_image.clahe,
                           visual=visual,
                           verbose=verbose)
    for card_contour in contours:
        try:
            (continue_segmentation,
             is_card_candidate,
             bounding_poly,
             crop_factor) = characterize_card_contour(card_contour,
                                                     max_segment_area,
                                                     image_area)
        except NotImplementedError as nie:
            # this can occur in Shapely for some funny contour shapes
            # resolve by discarding the candidate
            print(nie)
            (continue_segmentation,
             is_card_candidate,
             bounding_poly,
             crop_factor) = (True, False, None, 1.)
        if not continue_segmentation:
            break
        if is_card_candidate:
            if max_segment_area < 0.1:
                max_segment_area = bounding_poly.area
            warped = four_point_transform(full_image,
                                         scale(bounding_poly,
                                               xfact=crop_factor,
                                               yfact=crop_factor,
                                               origin=))
            test_image.candidate_list.append(
                CardCandidate(warped,
                             bounding_poly,
                             bounding_poly.area / image_area))
            if verbose:
                print( +
                      str(len(test_image.candidate_list)) +
                      )# File: lib/geometry/transforms.py
import numpy as np
import cv2
from shapely.geometry import LineString
from shapely.geometry.polygon import Polygon
from shapely.affinity import scale
def order_polygon_points(x, y):
    
    angle = np.arctan2(y - np.average(y), x - np.average(x))
    ind = np.argsort(angle)
    return (x[ind], y[ind])
def four_point_transform(image, poly):
    
    pts = np.zeros((4, 2))
    pts[:, 0] = np.asarray(poly.exterior.coords)[:-1, 0]
    pts[:, 1] = np.asarray(poly.exterior.coords)[:-1, 1]
    # obtain a consistent order of the points and unpack them
    # individually
    rect = np.zeros((4, 2))
    (rect[:, 0], rect[:, 1]) = order_polygon_points(pts[:, 0], pts[:, 1])
    # compute the width of the new image, which will be the
    # maximum distance between bottom-right and bottom-left
    # x-coordiates or the top-right and top-left x-coordinates
    width_a = np.sqrt(((rect[1, 0] - rect[0, 0]) ** 2) +
                      ((rect[1, 1] - rect[0, 1]) ** 2))
    width_b = np.sqrt(((rect[3, 0] - rect[2, 0]) ** 2) +
                      ((rect[3, 1] - rect[2, 1]) ** 2))
    max_width = max(int(width_a), int(width_b))
    # compute the height of the new image, which will be the
    # maximum distance between the top-right and bottom-right
    # y-coordinates or the top-left and bottom-left y-coordinates
    height_a = np.sqrt(((rect[0, 0] - rect[3, 0]) ** 2) +
                       ((rect[0, 1] - rect[3, 1]) ** 2))
    height_b = np.sqrt(((rect[1, 0] - rect[2, 0]) ** 2) +
                       ((rect[1, 1] - rect[2, 1]) ** 2))
    max_height = max(int(height_a), int(height_b))
    # now that we have the dimensions of the new image, construct
    # the set of destination points to obtain a "birds eye view",
    # (i.e. top-down view) of the image, again specifying points
    # in the top-left, top-right, bottom-right, and bottom-left
    # order
    rect = np.array([
        [rect[0, 0], rect[0, 1]],
        [rect[1, 0], rect[1, 1]],
        [rect[2, 0], rect[2, 1]],
        [rect[3, 0], rect[3, 1]]], dtype="float32")
    dst = np.array([
        [0, 0],
        [max_width - 1, 0],
        [max_width - 1, max_height - 1],
        [0, max_height - 1]], dtype="float32")
    # compute the perspective transform matrix and then apply it
    transform = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, transform, (max_width, max_height))
    # return the warped image
    return warped# File: lib/geometry/polygons.py
import numpy as np
import cv2
from shapely.geometry import LineString
from shapely.geometry.polygon import Polygon
from shapely.affinity import scale
from itertools import product
def line_intersection(x, y):
    
    slope_0 = (x[0] - x[1]) * (y[2] - y[3])
    slope_2 = (y[0] - y[1]) * (x[2] - x[3])
    if slope_0 == slope_2:
        # parallel lines
        xis = np.nan
        yis = np.nan
    else:
        xy_01 = x[0] * y[1] - y[0] * x[1]
        xy_23 = x[2] * y[3] - y[2] * x[3]
        denom = slope_0 - slope_2
        xis = (xy_01 * (x[2] - x[3]) - (x[0] - x[1]) * xy_23) / denom
        yis = (xy_01 * (y[2] - y[3]) - (y[0] - y[1]) * xy_23) / denom
    return (xis, yis)
def simplify_polygon(in_poly,
                     length_cutoff=0.15,
                     maxiter=None,
                     segment_to_remove=None):
    
    x_in = np.asarray(in_poly.exterior.coords)[:-1, 0]
    y_in = np.asarray(in_poly.exterior.coords)[:-1, 1]
    len_poly = len(x_in)
    niter = 0
    if segment_to_remove is not None:
        maxiter = 1
    while len_poly > 4:
        d_in = np.sqrt(np.ediff1d(x_in, to_end=x_in[0] - x_in[-1]) ** 2. +
                       np.ediff1d(y_in, to_end=y_in[0] - y_in[-1]) ** 2.)
        d_tot = np.sum(d_in)
        if segment_to_remove is not None:
            k = segment_to_remove
        else:
            k = np.argmin(d_in)
        if d_in[k] < length_cutoff * d_tot:
            ind = generate_point_indices(k - 1, k + 1, len_poly)
            (xis, yis) = line_intersection(x_in[ind], y_in[ind])
            x_in[k] = xis
            y_in[k] = yis
            x_in = np.delete(x_in, (k + 1) % len_poly)
            y_in = np.delete(y_in, (k + 1) % len_poly)
            len_poly = len(x_in)
            niter += 1
            if (maxiter is not None) and (niter >= maxiter):
                break
        else:
            break
    out_poly = Polygon([[ix, iy] for (ix, iy) in zip(x_in, y_in)])
    return out_poly
def generate_point_indices(index_1, index_2, max_len):
    
    return np.array([index_1 % max_len,
                     (index_1 + 1) % max_len,
                     index_2 % max_len,
                     (index_2 + 1) % max_len])
def generate_quad_corners(indices, x, y):
    
    (i, j, k, l) = indices
    def gpi(index_1, index_2):
        return generate_point_indices(index_1, index_2, len(x))
    xis = np.empty(4)
    yis = np.empty(4)
    xis.fill(np.nan)
    yis.fill(np.nan)
    if j <= i or k <= j or l <= k:
        pass
    else:
        (xis[0], yis[0]) = line_intersection(x[gpi(i, j)],
                                             y[gpi(i, j)])
        (xis[1], yis[1]) = line_intersection(x[gpi(j, k)],
                                             y[gpi(j, k)])
        (xis[2], yis[2]) = line_intersection(x[gpi(k, l)],
                                             y[gpi(k, l)])
        (xis[3], yis[3]) = line_intersection(x[gpi(l, i)],
                                             y[gpi(l, i)])
    return xis, yis
def generate_quad_candidates(in_poly):
    
    # make sure that the points are ordered
    from lib.geometry.transforms import order_polygon_points
    
    (x_s, y_s) = order_polygon_points(
        np.asarray(in_poly.exterior.coords)[:-1, 0],
        np.asarray(in_poly.exterior.coords)[:-1, 1])
    x_s_ave = np.average(x_s)
    y_s_ave = np.average(y_s)
    x_shrunk = x_s_ave + 0.9999 * (x_s - x_s_ave)
    y_shrunk = y_s_ave + 0.9999 * (y_s - y_s_ave)
    shrunk_poly = Polygon([[x, y] for (x, y) in
                           zip(x_shrunk, y_shrunk)])
    quads = []
    len_poly = len(x_s)
    for indices in product(range(len_poly), repeat=4):
        (xis, yis) = generate_quad_corners(indices, x_s, y_s)
        if (np.sum(np.isnan(xis)) + np.sum(np.isnan(yis))) > 0:
            # no intersection point for some of the lines
            pass
        else:
            (xis, yis) = order_polygon_points(xis, yis)
            enclose = True
            quad = Polygon([(xis[0], yis[0]),
                            (xis[1], yis[1]),
                            (xis[2], yis[2]),
                            (xis[3], yis[3])])
            if not quad.contains(shrunk_poly):
                enclose = False
            if enclose:
                quads.append(quad)
    return quads
def get_bounding_quad(hull_poly):
    
    simple_poly = simplify_polygon(hull_poly)
    bounding_quads = generate_quad_candidates(simple_poly)
    bquad_areas = np.zeros(len(bounding_quads))
    for iquad, bquad in enumerate(bounding_quads):
        bquad_areas[iquad] = bquad.area
    min_area_quad = bounding_quads[np.argmin(bquad_areas)]
    return min_area_quad
def quad_corner_diff(hull_poly, bquad_poly, region_size=0.9):
    
    bquad_corners = np.zeros((4, 2))
    bquad_corners[:, 0] = np.asarray(bquad_poly.exterior.coords)[:-1, 0]
    bquad_corners[:, 1] = np.asarray(bquad_poly.exterior.coords)[:-1, 1]
    # The point inside the quadrilateral, region_size towards the quad center
    interior_points = np.zeros((4, 2))
    interior_points[:, 0] = np.average(bquad_corners[:, 0]) + \
        region_size * (bquad_corners[:, 0] - np.average(bquad_corners[:, 0]))
    interior_points[:, 1] = np.average(bquad_corners[:, 1]) + \
        region_size * (bquad_corners[:, 1] - np.average(bquad_corners[:, 1]))
    # The points p0 and p1 (at each corner) define the line whose intersections
    # with the quad together with the corner point define the triangular
    # area where the roundness of the convex hull in relation to the bounding
    # quadrilateral is evaluated.
    # The line (out of p0 and p1) is constructed such that it goes through the
    # "interior_point" and is orthogonal to the line going from the corner to
    # the center of the quad.
    p0_x = interior_points[:, 0] + \
        (bquad_corners[:, 1] - np.average(bquad_corners[:, 1]))
    p1_x = interior_points[:, 0] - \
        (bquad_corners[:, 1] - np.average(bquad_corners[:, 1]))
    p0_y = interior_points[:, 1] - \
        (bquad_corners[:, 0] - np.average(bquad_corners[:, 0]))
    p1_y = interior_points[:, 1] + \
        (bquad_corners[:, 0] - np.average(bquad_corners[:, 0]))
    corner_area_polys = []
    for i in range(len(interior_points[:, 0])):
        bline = LineString([(p0_x[i], p0_y[i]), (p1_x[i], p1_y[i])])
        corner_area_polys.append(Polygon(
            [bquad_poly.intersection(bline).coords[0],
             bquad_poly.intersection(bline).coords[1],
             (bquad_corners[i, 0], bquad_corners[i, 1])]))
    hull_corner_area = 0
    quad_corner_area = 0
    for capoly in corner_area_polys:
        quad_corner_area += capoly.area
        hull_corner_area += capoly.intersection(hull_poly).area
    return 1. - hull_corner_area / quad_corner_area
def convex_hull_polygon(contour):
    
    hull = cv2.convexHull(contour)
    phull = Polygon([[x, y] for (x, y) in
                     zip(hull[:, :, 0], hull[:, :, 1])])
    return phull
def polygon_form_factor(poly):
    
    # minimum side length
    d_0 = np.amin(np.sqrt(np.sum(np.diff(np.asarray(poly.exterior.coords),
                                         axis=0) ** 2., axis=1)))
    return poly.area / (poly.length * d_0)
def characterize_card_contour(card_contour,
                             max_segment_area,
                             image_area):
    
    phull = convex_hull_polygon(card_contour)
    if (phull.area < 0.1 * max_segment_area or
            phull.area < image_area / 1000.):
        # break after card size range has been explored
        continue_segmentation = False
        is_card_candidate = False
        bounding_poly = None
        crop_factor = 1.
    else:
        continue_segmentation = True
        bounding_poly = get_bounding_quad(phull)
        qc_diff = quad_corner_diff(phull, bounding_poly)
        crop_factor = min(1., (1. - qc_diff * 22. / 100.))
        is_card_candidate = bool(
            0.1 * max_segment_area < bounding_poly.area <
            image_area * 0.99 and
            qc_diff < 0.35 and
            0.25 < polygon_form_factor(bounding_poly) < 0.33)
    return (continue_segmentation,
            is_card_candidate,
            bounding_poly,
            crop_factor)# File: lib/geometry/__init__.py
from lib.geometry.transforms import (
    order_polygon_points,
    four_point_transform,
)
from lib.geometry.polygons import (
    line_intersection,
    simplify_polygon,
    generate_point_indices,
    generate_quad_corners,
    generate_quad_candidates,
    get_bounding_quad,
    quad_corner_diff,
    convex_hull_polygon,
    polygon_form_factor,
    characterize_card_contour,
)